{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple GenAI App\n",
    "url = \"https://towardsdatascience.com/ai-agents-processing-timeseries-and-large-dataframes/\"\n",
    "# Scrape from website\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(url)\n",
    "docs = loader.load()\n",
    "# Load docs -> convert to chunks -> convert to embeddings -> store in vector DB\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "# These documents will be converted to vectors and stored in vector DB, becasue for RAG we fetch relevant similar content using cosine similarity which works on vectors\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore_db = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "vectorstore_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How LLMs work with smaller datasets?\"\n",
    "result = vectorstore_db.similarity_search(query, k=3)\n",
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    \"\"\"\n",
    "    Answer the following question based on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "    \"\"\"\n",
    "])\n",
    "\n",
    "# document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "print(document_chain)\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"How LLMs work with smaller datasets?\",\n",
    "    \"context\":[Document(page_content=\"LLMs are trained on large datasets, but they can also work with smaller datasets by leveraging transfer learning and fine-tuning techniques. This allows them to adapt to specific tasks or domains without requiring massive amounts of data. Additionally, LLMs can generate synthetic data to augment smaller datasets, improving their performance on tasks with limited data availability.\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "# Input -> Retriever -> Vector DB | Retriever is an interface to fetch relevant documents from vector DB\n",
    "retriever = vectorstore_db.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "retrieval_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\":\"what does Langchain do, some info about it?\"})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"What is the capital of France?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You're an expert AI Engineer. Please provide me answers based on the following questions\"),\n",
    "    (\"user\",\"{input}\"),\n",
    "])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=prompt|llm\n",
    "response = chain.invoke({\"input\":\"What is the capital of France?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt|llm|output_parser\n",
    "response = chain.invoke({\"input\":\"What is the capital of France?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
